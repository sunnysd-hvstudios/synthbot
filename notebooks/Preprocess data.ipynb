{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You do not need to do any of this\n",
    "This file walks you through the preprocessing steps for:\n",
    "1. Cleaning the data,\n",
    "2. Aligning transcripts to utterances at the phoneme level, and\n",
    "3. Packaging the data for data science and machine learning uses.\n",
    "\n",
    "**This is a slow, painful, and iterative process. If you're just interested in data science / machine learning, I recommend skipping this and downloading the end results.**\n",
    "\n",
    "If you want to help out with programmatic data cleaning, then this is for you. There's a lot of work to be done here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_type = 'clipper' # options: \"vctk\", \"clipper\"\n",
    "samples_dir = '/home/celestia/data/clipper-samples'\n",
    "preproc_dir = '/home/celestia/data/clipper-preproc'\n",
    "dictionary_dir = '/home/celestia/data/dictionaries'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert clipper-formatted data to mfa-formatted data\n",
    "The goal here is to run Montreal Forced Aligner (MFA) through Clipper's clips. Clipper's files are flac files and word-level transcripts. MFA takes in 16khz wave files and word-level transcripts, and it outputs phoneme-level transcripts. The `datapipes` module in `src/` can convert Clipper's files into MFA-compatible input.\n",
    "\n",
    "First step: do a dry-run to check for any errors in the Clipper files we have. Sometimes there's a filename mismatch, a missing character name, missing transcript file, or similar. While running this, you'll see the `In [ ]` on the left-hand side change to `In [*]`. When it's complete, you'll see it change to `In [1]`. The number `[1]` tells you the order in which commands on this page were executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-08 21:02:12.551126: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\n",
      "2020-03-08 21:02:12.552440: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "!(cd ../src; python -m datapipes --mfa-inputs \\\n",
    "    --input \"{samples_dir}\" `# clipper-formatted directory` \\\n",
    "    --output \"{preproc_dir}/mfa-inputs\" `# mfa-formatted directory` \\\n",
    "    --dry-run `# don't create any output files`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second step: build the necessary dictionaries. We could technically do this step later, but this is a good way to catch typos in the transcriptions, so we'll do it early. The dictionaries are used to determine how words are pronounced using Arpabet. No single dictionary contains all of the pronunciations we need, so we use two standard dictionaries (LibriSpeech and CMU Dict) plus a custom dictionary (Horsewords) that contains show-specific pronunciations. The standard dictionaries are built on the assumption that the speaker uses the \"correct\" enunciations, so the Horsewords dictionary also contains \"messy\" pronunciations, including stutters and slurred speech.\n",
    "\n",
    "The following command validates and merges the three dictionaries, then checks to make sure all words in Clipper's transcriptions have a pronunciation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-08 21:23:46.027661: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\r\n",
      "2020-03-08 21:23:46.028926: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6\r\n"
     ]
    }
   ],
   "source": [
    "!(cd ../src; python -m datapipes --dictionary \\\n",
    "     --include \"{dictionary_dir}\"/librispeech.txt \\\n",
    "         \"{dictionary_dir}\"/cmudict-0.7b.txt \\\n",
    "         \"{dictionary_dir}\"/horsewords.txt \\\n",
    "         \"{dictionary_dir}\"/gothicwords.txt \\\n",
    "         \"{dictionary_dir}\"/whovianwords.txt \\\n",
    "     --clipper-path \"{samples_dir}\" \\\n",
    "     --output-path \"{dictionary_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are any errors reported by the above two commands, make sure to fix them and re-run the commands. Repeat until there are no errors. The next command will generate the mfa-formatted data. If you're running this on all of Clipper's data, this might take an hour to complete. The `--delta` flag tells the script to only process new files. It does NOT tell the script to update existing changed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "rm -r /home/celestia/data/mfa-inputs.bak\n",
    "mv /home/celestia/data/mfa-inputs /home/celestia/data/mfa-inputs.bak\n",
    "\n",
    "(cd ../src; python -m datapipes --mfa-inputs \\\n",
    "    --input \"/home/celestia/data/clipper-samples\" \\\n",
    "    --output /home/celestia/data/mfa-inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run montreal-forced-aligner with the following command to generate phoneme-level transcripts. Note that, \n",
    "due to quirks with IPython, this command won't produce intermediate output, so you won't be able to monitor progress here. If you're running this on all of Clipper's data, this command might take a few hours to complete. You can monitor progress by watching the `data/mfa-alignments` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "rm -r /home/celestia/data/mfa-alignments.bak\n",
    "mv /home/celestia/data/mfa-alignments /home/celestia/data/mfa-alignments.bak\n",
    "rm -r /home/celestia/Documents/MFA\n",
    "\n",
    "function mfa() {\n",
    "    mkdir /home/celestia/data/mfa-alignments/$1 || true\n",
    "    yes n | mfa_align -v `# continue even with an incomplete dictionary` \\\n",
    "        /home/celestia/data/mfa-inputs/$1 `# input directory` \\\n",
    "        /home/celestia/data/dictionaries/normalized.dict.txt \\\n",
    "        /opt/mfa/pretrained_models/english.zip \\\n",
    "        /home/celestia/data/mfa-alignments/$1 `# output directory` \\\n",
    "        || true\n",
    "}\n",
    "\n",
    "export -f mfa\n",
    "\n",
    "ls /home/celestia/data/mfa-inputs | xargs -L1 -P16 bash -c 'mfa $@' _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's extremely likely that MFA failed on some inputs. There are three ways in which it can fail:\n",
    "1. MFA found a word it didn't recognize and logged both the missing word and corresponding utterance.\n",
    "2. MFA failed in some unexpected way while doing preprocessing for a character, and it borked its own configuration files.\n",
    "3. MFA couldn't figure out how to align the transcript to an utterance.\n",
    "\n",
    "For the first kind of failure, you can find an `oovs_found.txt` file in each of the directories within `mfa-alignments`. This file contains a list of words that could not be processed because they don't exist in the pronunciation dictionary. You can find the current pronunciation dictionary in `/opt/mfa/pronunciations_dicts/english.dict.txt`. If you end up adding the pronunciations of any missing words, make sure to post them to the thread. I can update the Docker image so everyone can benefit from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat /home/celestia/data/mfa-alignments/*/oovs_found.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second and third kinds of failure, you can find out which characters MFA failed to process by searching for the empty directories  in `mfa-alignments`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "find /home/celestia/data/mfa-alignments -type d -empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above command produces any output, it's very likely that MFA stochastically borked something during its own preprocessing stage. The easiest way to handle this is to remove its character-specific cache directory and try again.\n",
    "\n",
    "The following script does exactly that for the case where MFA fails on Applejack's files. In my case, I needed to run this for Apple-Bloom, Applejack, Cadance, and Rainbow-Dash the most recent time, but MFA's failures are pretty stochastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "retry_character=\"Applejack\"\n",
    "\n",
    "rm -r \"/home/celestia/Documents/MFA/$retry_character\"\n",
    "\n",
    "yes n | mfa_align -v \\\n",
    "        /home/celestia/data/mfa-inputs/$retry_character \\\n",
    "        /opt/mfa/pronunciations_dicts/english.dict.txt \\\n",
    "        /opt/mfa/pretrained_models/english.zip \\\n",
    "        /home/celestia/data/mfa-alignments/$retry_character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last type of MFA failure is the only one that's complicated to handle. If you run the following command, you can see a list of transcripts that MFA failed to align."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "function get_textgrids() {\n",
    "    (cd \"$1\"\n",
    "    find -iname '*.textgrid' |\n",
    "        sed 's/\\.textgrid$//gI' |\n",
    "        sort)\n",
    "}\n",
    "\n",
    "diff <(get_textgrids /home/celestia/data/mfa-inputs) <(get_textgrids /home/celestia/data/mfa-alignments) || true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you didn't complete the above steps for handling whole-character issues, you'll notice that some characters have a huge number of utterances listed. If you did complete the above steps, none of the characters should have _that_ many failures. For me, the worst offender is Pinkie Pie with 77 failures, followed by Fluttershy and Twilight Sparkle both with around 35. If a character has a huge number of utterances listed, it's likely that MFA crashed at some point. You can read through its logs in `/home/celestia/Documents/MFA/` to try to figure out why. \n",
    "\n",
    "You can try playing some of the listed files to figure out why MFA might be failing on them. I've found that it's often because either (1) the character is speaking in a very excited or abnormal way, (2) the clip is noisy or muffled, or (3) the utterance contains a lot of out-of-dictionary words.\n",
    "\n",
    "Eventually, we'll want to find a way to make use of these utterances to generate more realistic speech in niche cases, but for now we can ignore them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package the audio and label data into a tarfile. We're using an uncompressed format because it's much faster to load the uncompressed version, and because compression doesn't save much space with these audio files.\n",
    "\n",
    "The audio format and sampling rate of the saved files can be modified to anything `pysoundfile` can handle. For now, we're using the most common sampling rate (48khz) and file format (wav) used by Clipper.\n",
    "\n",
    "This first command does a dry run to make sure there are no errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "(cd ../src; python -m datapipes --audio-tar \\\n",
    "    --input-audio /home/celestia/data/clipper-samples `# clipper-formatted directory` \\\n",
    "    --input-alignments /home/celestia/data/mfa-alignments `# mfa-formatted directory` \\\n",
    "    --output /home/celestia/data/audio-tar `# output per-character tar.gz files here` \\\n",
    "    --audio-format 'wav' \\\n",
    "    --sampling-rate 48000 \\\n",
    "    --dry-run `# don't create any output files` \\\n",
    "    --verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second command creates the tar archive files with the audio and label files. This is what most people use in the Colab notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "rm -r /home/celestia/data/audio-tar.bak\n",
    "mv /home/celestia/data/audio-tar /home/celestia/data/audio-tar.bak\n",
    "\n",
    "(cd ../src; python -m datapipes --audio-tar \\\n",
    "    --input-audio /home/celestia/data/clipper-samples \\\n",
    "    --input-alignments /home/celestia/data/mfa-alignments \\\n",
    "    --output /home/celestia/data/audio-tar \\\n",
    "    --audio-format 'wav' \\\n",
    "    --sampling-rate 48000 \\\n",
    "    --verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package extra phonetics data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some sound features that are commonly used in phonetics research. This includes pitch, volume, and formant information. Glottal Closure Instants are also commonly used to identify whether a speech segment is voiced or unvoiced. The following command extracts these features from the audio-tar files created above and writes them to lzma-compressed tar (txz) files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "rm -r /home/celestia/data/audio-info.bak\n",
    "mv /home/celestia/data/audio-info /home/celestia/data/audio-info.bak\n",
    "\n",
    "function generate_info() {\n",
    "    source=\"$(readlink -f $1)\"\n",
    "    target=\"/home/celestia/data/audio-info/$(basename $source .tar).txz\"\n",
    "    (cd ../src; python -m datapipes --audio-info \\\n",
    "        --input-tar \"$source\" \\\n",
    "        --output-txz \"$target\" \\\n",
    "        --verbose) || true\n",
    "}\n",
    "\n",
    "export -f generate_info\n",
    "\n",
    "ls /home/celestia/data/audio-tar/* | xargs -L1 -P16 bash -c 'generate_info $@' _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with Tensorflow, packaging data as into TFRecord files simplifies the process of loading data, especially when working with multiple GPUs. The following command packages all of the above label files (Clipper's labels, MFA phoneme transcriptions, and phonetics features) into TFRecord files. This can be used to create label embeddings.\n",
    "\n",
    "It's recommended practice to only store 100MB to 200MB of data per file. The generated TFRecord files will contain information on up to 24,000 clips, a number that's heuristically estimated to contain 150MB of data. Each generated file will end with \".tfrecordN\", where N is a number starting from 0. Currently, none of the characters have more than 24,000 clips, so every character's file ends with \".tfrecord0\".\n",
    "\n",
    "You can use the same command to generate a set of 100MB-200MB files containing all clips all mixed together. This would make more sense for training cross-character models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project-specific packaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "function generate_tfrecords() {\n",
    "    audio=\"$(readlink -f $1)\"\n",
    "    info=\"/home/celestia/data/audio-info/$(basename $audio .tar).txz\"\n",
    "    tfrecord=\"/home/celestia/data/labels-tfrecord/$(basename $audio .tar).tfrecord\"\n",
    "    (cd ../src; python -m datapipes --labels-tfrecord \\\n",
    "        --input-labels \"$audio\" \\\n",
    "        --input-info \"$info\" \\\n",
    "        --output-tfrecord \"$tfrecord\" \\\n",
    "        --verbose) || true\n",
    "}\n",
    "\n",
    "export -f generate_tfrecords\n",
    "\n",
    "ls /home/celestia/data/audio-tar/* | xargs -L1 -P16 bash -c 'generate_tfrecords $@' _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cookie-normalized data\n",
    "Cookie's (initial) scripts require the clips to be 22khz and trimmed. The following script was used before to package clips as 48khz files. We'll use the same script to create the 22khz files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "rm -r /home/celestia/data/audio-tar-22khz.bak\n",
    "mv /home/celestia/data/audio-tar-22khz /home/celestia/data/audio-tar-22khz.bak\n",
    "\n",
    "(cd ../src; python -m datapipes --audio-tar \\\n",
    "    --input-audio /home/celestia/data/clipper-samples \\\n",
    "    --input-alignments /home/celestia/data/mfa-alignments \\\n",
    "    --output /home/celestia/data/audio-tar-22khz \\\n",
    "    --audio-format 'wav' \\\n",
    "    --sampling-rate 22050 \\\n",
    "    --verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can trim the 22khz audio with the following `sox` command. (Thanks, Cookie!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "rm -r /home/celestia/data/audio-trimmed\n",
    "mv /home/celestia/data/audio-trimmed /home/celestia/data/audio-trimmed.bak\n",
    "\n",
    "function trim_character() {\n",
    "    input_tar=\"$(readlink -f $1)\"\n",
    "    output_folder=\"/home/celestia/data/audio-trimmed/$(basename $input_tar .tar)\"\n",
    "    echo \"writing to $output_folder\"\n",
    "    mkdir -p \"$output_folder\"\n",
    "    (cd \"$output_folder\"\n",
    "        tar xf \"$input_tar\";\n",
    "        for clip in `ls`; do\n",
    "            cp \"$clip/audio.wav\" \"tmp.wav\";\n",
    "            sox \"tmp.wav\" \"$clip/audio.wav\" silence 1 0.05 0.1% reverse silence 1 0.05 0.1% reverse;\n",
    "        done\n",
    "        \n",
    "        rm \"tmp.wav\"\n",
    "    )\n",
    "}\n",
    "\n",
    "export -f trim_character\n",
    "\n",
    "ls /home/celestia/data/audio-tar-22khz/* | xargs -L1 -P16 bash -c 'trim_character $@' _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally package the results into `tar` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "(cd /home/celestia/data/audio-trimmed\n",
    " pwd\n",
    " for character in `ls`; do\n",
    "     (cd \"$character/\"\n",
    "      tar -cf \"../$character.tar\" */audio.wav */label.json)\n",
    " done)\n",
    "\n",
    "rm -r /home/celestia/data/audio-trimmed-22khz.bak\n",
    "mv /home/celestia/data/audio-trimmed-22khz /home/celestia/data/audio-trimmed-22khz.bak\n",
    "\n",
    "mkdir /home/celestia/data/audio-trimmed-22khz\n",
    "mv /home/celestia/data/audio-trimmed/*.tar /home/celestia/data/audio-trimmed-22khz/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.6.9, pytest-5.3.2, py-1.8.1, pluggy-0.13.1\n",
      "rootdir: /home/celestia/synthbot/src\n",
      "plugins: cov-2.8.1\n",
      "collected 6 items\n",
      "\n",
      "src ......                                                               [100%]\n",
      "\n",
      "======================== 6 passed in 370.81s (0:06:10) =========================\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "(cd ../; pytest --rootdir=src/ tests/test_datafiles.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push the data to Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "local=\"/home/celestia/data\"\n",
    "remote_hidden=\"/home/celestia/drive/soundtools-alpha/\"\n",
    "\n",
    "rm $remote_hidden/audio-tar.new/*\n",
    "cp $local/audio-tar/* $remote_hidden/audio-tar.new/\n",
    "\n",
    "rm $remote_hidden/audio-info.new/*\n",
    "cp $local/audio-info/* $remote_hidden/audio-info.new/\n",
    "\n",
    "rm $remote_hidden/audio-trimmed-22khz.new/*\n",
    "cp $local/audio-trimmed-22khz/* $remote_hidden/audio-trimmed-22khz.new/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
